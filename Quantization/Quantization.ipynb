{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8626572-a1ac-4762-9aac-273bb970ef2c",
   "metadata": {},
   "source": [
    "# What is Quantization?\n",
    "\n",
    " - Quantization is a technique that reduces the numerical precision of model parameters, optimizing memory usage and computational efficiency.\n",
    " - Instead of relying on 32-bit floating-point numbers (float32), models can be optimized using lower-precision formats like 16-bit floating-point (float16) or even 8-bit integers (int8).\n",
    " - This process can significantly shrink model size and accelerate inference while keeping accuracy at an acceptable level.\n",
    "\n",
    "# Why Use Quantization for LLMs?\n",
    "\n",
    " - Enables the deployment of large-scale models on devices with constrained hardware resources.\n",
    " - Preserves computational performance without introducing major accuracy losses.\n",
    " - Enhances the ability to run LLMs on mobile devices and in real-time applications.\n",
    " - A practical solution for users running models on cloud-based platforms with resource limitations, such as Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3950bb-3159-4dec-9047-22b2d445ce51",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers einops accelerate bitsandbytes torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f3d98a-ab8d-488c-8163-78f2422a6314",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
    "import torch\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbd986b-2a32-4bdf-8d50-7a8c93fba108",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HF_TOKEN\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93b7dfb-d853-4c40-8a70-c4ddcaed64c0",
   "metadata": {},
   "source": [
    "# Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0771e43d-2826-4ca7-865d-fcb886743715",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9238670-8a71-43a5-a975-5bfc054be250",
   "metadata": {},
   "source": [
    "### Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab61f73a-1f3b-4714-93db-78130381cea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1322ee-8d37-4da2-8737-344eb1d88a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = (\"Who was the first person in space?\")\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b338c1-9664-47a7-aa44-c1bd5e88867b",
   "metadata": {},
   "outputs": [],
   "source": [
    "encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "model_inputs = encodeds.to(device)\n",
    "generated_ids = model.generate(model_inputs, max_new_tokens = 1000, do_sample = True,\n",
    "                               pad_token_id=tokenizer.eos_token_id)\n",
    "decoded = tokenizer.batch_decode(generated_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9401ce2-4fad-48d2-b31d-38efba0404a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = decoded[0]\n",
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
